1
00:00:00,000 --> 00:04:05,000
Okay, so this is a demo which is going to combine in Genie Hub with our lab and then our Flame Cloud environment to show how we can get files in and out of that Flame environment in AWS. So to recap, this is the pick store running out of AWS. We've got two storage nodes and a flame client. We have our pick store interface here. So this cloud environment has a single target, which is an S3 bucket. So if we look at that, so this is going to make sure that any files in the folder, data slash flame, or slash ingenia will be sent into our demo bucket. So that is the cloud environment. And if I take a look at the lab, so if I go over to here, we've got our production lab environment, so loads of stuff going on, but it's the big cluster in the lab. And the if I going to this flam here, this folder here, we have some test frames. And as you saw just a second ago, it's based on the Netflix Open Source content, so this is a really nice full frame sort of media that we've been using for testing this workflow. I'm just going to flip over to now I'm going to use DCV viewer, because this is how we get into the Flame workstation itself and then I'm just going to copy any IP address. Log in. So this is the Autodesk supplied Flame workstation. If I get to places now, I'm going to go to my computer. We've got MMFS1 now mounted. So MMFS 1 data, flame 0,1, and at the moment. So I've got some example footage up there, but I'm actually going to send some different frames. So I'm gonna receive my working on a completely separate project now. It's not a NIST-Test folder, but it's somewhere else. So we have a pick store on the cloud, and we're gonna use a junior hub to get some files between each of those places. So I have a worker running at each of the sites, and if I just scroll down here, so we've got two size AWS UK Flame, and this is browsing the file system. And as we can see, we've that in junior folder. There's my TI folder and if I go right down to the bottom here, I've got the production lab. Now this has got tons of data on it, but the data I care about here is in the Flame 1 in junior directory and it's actually in my DCI 4K tests. So I've got some of those Netflix frames and you can see Hub has bundled them into sequences of 100 files because it's recognized as a pattern in the final name. So I'm just going to grab the first, I don't know, 400. It's 24 frames a second footage. So let's take 500 and then we're going to use the send workflow. So scroll down to the bottom and we are going to send these to the AWS Flame site. I want them to hydrate our destination. It will take a little bit longer, but then those frames are going to be ready for Flame to do its business, to play back off the Pixdore in the Cloud storage. So this is going to create a nice big job. It's going to take a couple of minutes to execute, but it's going migrate all of these files into the cloud. So I'm going to go out mute very quickly. I'll pause recording. I think that should save some time. But yeah, we'll just watch this job. And yeah, back in a second. OK, so after a little while, we've started to see the progress bar. So 501 frames, 50 mega frames, so we've shunted about 25 gigs with the data. The runtime on that in hub we now see a runtime for the job. So we're looking, let's have a look here, we're looking at about five to ten minutes. Now some of that data was already in the bucket. So I'll give myself a little bit of a head start, but you'll see as we start to migrate these frames up, we'll see some progress there. And yeah, let's look at that runtime. I'll just leave that there or pong. So if I'm mute. So we'll do that success. There you go. Five minutes and 10 seconds.

2
00:04:05,000 --> 00:07:29,879
Okay. So we'll do that success. There you go. Five minutes and 10 seconds. Okay. So if I go back to the main interface, and if I browse the AWS Flame now, so if I got into my media files here, my DCI 4K test footage is now in the cloud. I've got my frames there so I can head over to DCV viewer. I will just log into the box. I'll just give that direct to a quick free fresh. You can see we've got DCI4K test as I scroll down. Linux is going to do its thumbnail thing but fundamentally we should be ready to go and load up Flame. I'm going to go to Autodesk here, launch the Flame user interface and I'm going to create myself a brand new project. I am going to call it 4K demo and we're going to set that to MMFS1 data. So we're using DCP4K4 1.9, which is 3840 by 2160. That's in some right DCP. Yeah, 496 by 21 60 there we go. That is the proper frame size. 16 bits, 23.98 frames a second and I think we are good to go so we're going to create that project and I'll start there. Now once frame is loaded up I'm going to go into the media hub view which I think is this keyboard shortcut here so So I'm just gonna browse my local file system, go to that engineer folder. Here's my DCI4K test. I'm gonna just gonna drag it into here. I'm going to just gonna load it as a sequence. So, we should see there you go. Man walking up a hill, all looking good. Now I'm going to do a couple of things, I'm gonna put it into loop playback and here's a keyboard shortcut to bring up the debug monitor and that'll appear when we are playing and I'm also gonna bring up a resource manager just pop that over there. So now me hit play. So you can see what we're looking at is the video just drop frames. So that is the question of, are we getting the frames from the pick store into the playback buffer? And then if we leave this playing again, we can also see that we've got all of those other debug indicators. But from a storage perspective, we want to make sure we're not choking the application. So this is playing around in loop and it's all going nicely. If I head over to the admin interface for this pick store, the one that's running in the cloud, if I go to monitoring now, and if I'm going to the network view, I should see a whole bunch of redactivity on the storage. And as this levels off, it looks like

3
00:07:29,879 --> 00:07:33,959
we're peaking about 600 megabytes per storage node, which makes

4
00:07:33,959 --> 00:07:36,680
sense because this material is about 1.2 gigabytes a second.

5
00:07:37,240 --> 00:07:40,920
There's an overall bandwidth. I'll just pop back. And it's

6
00:07:40,920 --> 00:07:47,920
playing away quite happily, just doing its thing. And we're seeing nice consistent read performance of the storage.

7
00:07:49,600 --> 00:07:56,879
So that was some files I had on-prem. I pushed up to the cloud and I just used an genie I had to do that.

